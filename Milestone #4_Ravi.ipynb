{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0315c124",
   "metadata": {},
   "source": [
    "# Train Fake News Detection CLIP Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b413dd8",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e00a196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "## Ensure TensorFlow is not used\n",
    "import os\n",
    "os.environ[\"USE_TF\"] = \"0\"\n",
    "\n",
    "# Define data directory\n",
    "DATA_DIR = \"cleaned_data\"\n",
    "\n",
    "# Define file paths\n",
    "TRAIN_DATA_FILE = os.path.join(DATA_DIR, \"train.csv\")\n",
    "VALIDATION_DATA_FILE = os.path.join(DATA_DIR, \"validation_5k.csv\")\n",
    "TEST_DATA_FILE = os.path.join(DATA_DIR, \"test_5k.csv\")\n",
    "\n",
    "# For reproducability\n",
    "random_state = 42\n",
    "\n",
    "# Use CPU/MPS if possible\n",
    "device = None\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # Running in Colab\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "else:\n",
    "    # Not in Colab (e.g., Mac)\n",
    "    device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1533e8",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86261439",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = pd.read_csv(TRAIN_DATA_FILE)\n",
    "VALIDATION_DATA = pd.read_csv(VALIDATION_DATA_FILE, index_col = 0)\n",
    "TEST_DATA = pd.read_csv(TEST_DATA_FILE, index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e8525e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore rows in corrupted_indices.txt files\n",
    "def filter_out_corrupted_rows(split, DF):\n",
    "    # File with corrupted indices\n",
    "    if split == \"train\":\n",
    "        corrupted_indices_file = f\"{split}_corrupted_indices.txt\"\n",
    "    else:\n",
    "        corrupted_indices_file = f\"{split}_5k_corrupted_indices.txt\"\n",
    "\n",
    "    # Store list of corrupted indices\n",
    "    corrupted_indices = None\n",
    "\n",
    "    # Get list of corrupted indices\n",
    "    with open(corrupted_indices_file, \"r\") as f:\n",
    "        corrupted_indices = list(int(line.strip()) for line in f if line.strip())\n",
    "\n",
    "    print(f\"Split: {split}, Corrupted Indices: {corrupted_indices}, Length: {len(corrupted_indices)}\")\n",
    "\n",
    "    # Filter out corrupted rows\n",
    "    DF = DF.drop(index = corrupted_indices)\n",
    "\n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9fdbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = filter_out_corrupted_rows(\"train\", TRAIN_DATA)\n",
    "VALIDATION_DATA = filter_out_corrupted_rows(\"validation\", VALIDATION_DATA)\n",
    "TEST_DATA = filter_out_corrupted_rows(\"test\", TEST_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844d0c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA['image_num'] = TRAIN_DATA.index.astype(str).str.zfill(5)\n",
    "VALIDATION_DATA['image_num'] = VALIDATION_DATA.index.astype(str).str.zfill(5)\n",
    "TEST_DATA['image_num'] = TEST_DATA.index.astype(str).str.zfill(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51548a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf021422",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_DATA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d697faa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb63096",
   "metadata": {},
   "source": [
    "## Create News Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4415b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedditDataset(Dataset):\n",
    "    def __init__(self, df, image_dir, transform=None):\n",
    "        self.df = df\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # Fetch text\n",
    "        text = row['clean_title']\n",
    "\n",
    "        # Get image number\n",
    "        image_num = row['image_num']\n",
    "\n",
    "        # Fetch Image\n",
    "        img_path = os.path.join(self.image_dir, f\"{image_num}.jpg\")\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Label\n",
    "        label = torch.tensor(row['2_way_label'], dtype=torch.long)\n",
    "\n",
    "        return text, image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1d376a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_test_tfms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1850e504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    texts, images, labels = zip(*batch)\n",
    "    images = torch.stack(images)\n",
    "    labels = torch.stack(labels)\n",
    "    return list(texts), images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9015c427",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RedditDataset(\n",
    "    df = TRAIN_DATA, \n",
    "    image_dir = \"train_images\",\n",
    "    transform = train_tfms\n",
    ")\n",
    "\n",
    "validation_dataset = RedditDataset(\n",
    "    df = VALIDATION_DATA, \n",
    "    image_dir = \"validation_images\",\n",
    "    transform = val_test_tfms\n",
    ")\n",
    "\n",
    "test_dataset = RedditDataset(\n",
    "    df = TEST_DATA, \n",
    "    image_dir = \"test_images\",\n",
    "    transform = val_test_tfms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a47aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=B,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "validation_loader = DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size=B,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=B,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd482a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sanity Check\n",
    "texts, imgs, labels = next(iter(train_loader))\n",
    "print(\"Number of texts:\", len(texts))\n",
    "print(\"Example text:\", texts[0])\n",
    "print(\"Images shape:\", imgs.shape)      # (B, 3, 224, 224)\n",
    "print(\"Labels shape:\", labels.shape)\n",
    "print(\"Labels:\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3590e75",
   "metadata": {},
   "source": [
    "## Set up Fake News Detection (FND) CLIP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e40c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ensure TensorFlow is not used\n",
    "import os\n",
    "os.environ[\"USE_TF\"] = \"0\"\n",
    "\n",
    "# Import necessary software\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models as tv_models\n",
    "from transformers import CLIPModel, BertModel, BertTokenizer, CLIPTokenizer\n",
    "\n",
    "# Use CPU/MPS if possible\n",
    "import sys\n",
    "device = None\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # Running in Colab\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "else:\n",
    "    # Not in Colab (e.g., Mac)\n",
    "    device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Projection Head: 2-layer MLP (Reference: Page 4, Figure 2 of Paper)\n",
    "class ProjectionHead(nn.Module):\n",
    "    # in_dim: Number of input features to the Projection Head\n",
    "    # hidden_dim: Size of hidden state representation\n",
    "    # out_dim: Size of output dimension\n",
    "    # dropout: dropout rate\n",
    "    def __init__(self, in_dim, hidden_dim=256, out_dim=64, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        # Sequence 1: FC -> BN -> ReLU\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Define Dropout \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Sequence 2: FC -> BN -> ReLU\n",
    "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(out_dim)\n",
    "\n",
    "    # Input Shape: (B, D) where B is batch size and D is in_dim\n",
    "    def forward(self, x):\n",
    "        # Sequence 1: FC -> BN -> ReLU\n",
    "        x = self.fc1(x) # Shape: B x hidden_dim\n",
    "        x = self.bn1(x) # Shape: B x hidden_dim\n",
    "        x = self.relu(x) # Shape: B x hidden_dim\n",
    "\n",
    "        # Dropout\n",
    "        x = self.dropout(x) # Shape: B x hidden_dim\n",
    "\n",
    "        # Sequence 2: FC -> BN -> ReLU\n",
    "        x = self.fc2(x) # Shape: B x out_dim\n",
    "        x = self.bn2(x) # Shape: B x out_dim\n",
    "        x = self.relu(x) # Shape: B x out_dim\n",
    "\n",
    "        # Return Output\n",
    "        return x # Shape: B x out_dim\n",
    "\n",
    "# Modality-Wise Attention\n",
    "class ModalityWiseAttention(nn.Module):\n",
    "    # feat_dim = L (feature length) of each modality\n",
    "    def __init__(self, feat_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # Store feature dimension\n",
    "        self.feat_dim = feat_dim\n",
    "        \n",
    "        # Define the MLP Components\n",
    "        self.fc1 = nn.Linear(3, 3)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc2 = nn.Linear(3, 3)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    # m_txt: Text Features, Shape: B (batch size) x L (number of features)\n",
    "    # m_img: Image Features, Shape: B (batch size) x L (number of features)\n",
    "    # m_multi: Multimodal Features, Shape: B (batch size) x L (number of features)\n",
    "    def forward(self, m_txt, m_img, m_multi):\n",
    "        # Use unsqueeze to change shapes\n",
    "        m_txt = torch.unsqueeze(m_txt, dim = -1) # Shape: B (batch size) x L (number of features) x 1\n",
    "        m_img = torch.unsqueeze(m_img, dim = -1) # Shape: B (batch size) x L (number of features) x 1\n",
    "        m_multi = torch.unsqueeze(m_multi, dim = -1) # Shape: B (batch size) x L (number of features) x 1\n",
    "\n",
    "        # Concatenate all modalities\n",
    "        x = torch.cat([m_txt, m_img, m_multi], dim = -1)  # (B, L, 3)\n",
    "\n",
    "        # Global average pooling\n",
    "        global_avg_pool = torch.mean(x, dim = 1) # Shape: (B, 3)\n",
    "\n",
    "        # Global max pooling\n",
    "        global_max_pool, _ = torch.max(x, dim = 1) # Shape: (B, 3)\n",
    "\n",
    "        # Summation of pooled vectors to get initial attention weights\n",
    "        x = global_avg_pool + global_max_pool # Shape: (B, 3)\n",
    "\n",
    "        # Pass through MLP w/ GeLU Activation\n",
    "        x = self.gelu(self.fc1(x)) # Shape: (B, 3)\n",
    "        x = self.gelu(self.fc2(x)) # Shape: (B, 3)\n",
    "        x = self.sigmoid(x) # Shape: (B, 3)\n",
    "\n",
    "        # Get final attention weights\n",
    "        w_txt = x[:, 0].unsqueeze(1).unsqueeze(2) # Shape: (B, 1, 1)\n",
    "        w_img = x[:, 1].unsqueeze(1).unsqueeze(2) # Shape: (B, 1, 1)\n",
    "        w_multi = x[:, 2].unsqueeze(1).unsqueeze(2) # Shape: (B, 1, 1)\n",
    "\n",
    "        # sum along modality to get aggregated feature\n",
    "        mAgg = w_txt * m_txt + w_img * m_img + w_multi * m_multi  # (B, L, 1)\n",
    "        mAgg = torch.squeeze(mAgg, dim = -1) # Shape: (B, L)\n",
    "        return mAgg\n",
    "\n",
    "# Define Final Classification Head\n",
    "class ClassificationHead(nn.Module):\n",
    "    # in_dim: Number of input features to the Classification Head\n",
    "    # hidden_dim: Size of hidden state representation\n",
    "    # out_dim: Size of output dimension\n",
    "    def __init__(self, in_dim, hidden_dim = 64, out_dim = 2):\n",
    "        super().__init__()\n",
    "\n",
    "        # Sequence 1: FC -> ReLU\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Sequence 2: FC\n",
    "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
    "    \n",
    "    # Shape of x: B(batch size) x d(# of features)\n",
    "    def forward(self, x):\n",
    "        # Pass through first layer\n",
    "        x = self.relu(self.fc1(x))\n",
    "\n",
    "        # Pass through second layer\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # Return output\n",
    "        return x\n",
    "\n",
    "# Fake News Detection(FND) CLIP Model\n",
    "class FND_CLIP(nn.Module):\n",
    "    # resnet_model_name: Name of resnet model\n",
    "    # clip_model_name: Name of CLIP model\n",
    "    # bert_model_name: Name of BERT Model\n",
    "    def __init__(\n",
    "        self,\n",
    "        resnet_model_name = \"resnet101\",\n",
    "        clip_model_name='openai/clip-vit-base-patch32',\n",
    "        bert_model_name='bert-base-uncased',\n",
    "        proj_hidden=256,\n",
    "        proj_out=64,\n",
    "        classifier_hidden=64,\n",
    "        dropout=0.2,\n",
    "        momentum=0.1\n",
    "    ):\n",
    "        super().__init__()   \n",
    "\n",
    "        # Sanity Check\n",
    "        assert resnet_model_name == \"resnet101\"\n",
    "\n",
    "        # 1. Setup ResNet Image Encoder\n",
    "        # Replace the final fully connected layer with Identity because we only need the ResNet feature embeddings.\n",
    "        self.image_encoder = tv_models.resnet101(weights='IMAGENET1K_V1')\n",
    "        self.image_encoder.fc = nn.Identity()\n",
    "\n",
    "        # Sanity Check: Assert that ResNet parameters are going to be fine tuned\n",
    "        for param in self.image_encoder.parameters():\n",
    "            assert param.requires_grad == True\n",
    "\n",
    "        # 2. Setup BERT Text Encoder\n",
    "        self.text_encoder = BertModel.from_pretrained(bert_model_name)\n",
    "        self.text_encoder_tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "\n",
    "        # Freeze BERT weights\n",
    "        for param in self.text_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # 3. Setup Multimodal (Text + Image) Encoder\n",
    "        self.multimodal_encoder = CLIPModel.from_pretrained(clip_model_name)\n",
    "        self.multimodal_encoder_tokenizer = CLIPTokenizer.from_pretrained(clip_model_name)\n",
    "\n",
    "        # Freeze CLIP weights\n",
    "        for param in self.multimodal_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # 4. Set up Text Projection Head\n",
    "        self.pTxt = ProjectionHead(in_dim = 1280, hidden_dim = proj_hidden, out_dim = proj_out, dropout = dropout)\n",
    "        self.pImg = ProjectionHead(in_dim = 2560, hidden_dim = proj_hidden, out_dim = proj_out, dropout = dropout)\n",
    "        self.pMix = ProjectionHead(in_dim = 1024, hidden_dim = proj_hidden, out_dim = proj_out, dropout = dropout)\n",
    "\n",
    "        # 5. Set up Modality-Wise Attention\n",
    "        self.attention = ModalityWiseAttention(feat_dim = proj_out)\n",
    "\n",
    "        # 6. Set up Final Classification Head\n",
    "        self.classification_head = ClassificationHead(in_dim = proj_out, hidden_dim = classifier_hidden, out_dim = 2)\n",
    "\n",
    "        # Set up Running Buffers\n",
    "        self.momentum = momentum\n",
    "        self.eps = 1e-8\n",
    "        self.register_buffer(\"running_mean\", torch.tensor(0.0, device=device))\n",
    "        self.register_buffer(\"running_var\", torch.tensor(1.0, device=device))\n",
    "    \n",
    "    # Shape: fCLIP_T (B, 512)\n",
    "    # Shape: fCLIP_I (B, 512)\n",
    "    def compute_multimodal_features(self, fCLIP_T, fCLIP_I):\n",
    "        sim = F.cosine_similarity(fCLIP_T, fCLIP_I) # Compute cosine similarity, Shape: (B, )\n",
    "        fMix = torch.cat((fCLIP_T, fCLIP_I), dim = 1) # Shape: (B, 512 + 512 = 1024)\n",
    "        \n",
    "        if self.training:\n",
    "            batch_mean = sim.mean() # Mean\n",
    "            batch_var = sim.var() # Variance\n",
    "\n",
    "            # update running stats\n",
    "            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean\n",
    "            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var\n",
    "\n",
    "            mean, var = batch_mean, batch_var\n",
    "        else:\n",
    "            # use running stats for eval\n",
    "            mean, var = self.running_mean, self.running_var\n",
    "        \n",
    "        # standardize similarity\n",
    "        sim_std = (sim - mean) / torch.sqrt(var + self.eps)\n",
    "\n",
    "        # weight multimodal features\n",
    "        sim_weight = torch.sigmoid(sim_std).unsqueeze(1) # Shape: (B, 1)\n",
    "\n",
    "        mMix = sim_weight * self.pMix(fMix) # Shape: (B, 64)\n",
    "\n",
    "        # Return fMix and mMix\n",
    "        return fMix, mMix\n",
    "\n",
    "    # txt(B, ), List of Text Strings\n",
    "    # img(B, C_in = 3, H_in = 224, W_in = 224), List of Corresponding Imagess\n",
    "    def forward(self, txt, img):\n",
    "        # Compute BERT Text Features\n",
    "        text_encoding = self.text_encoder_tokenizer(\n",
    "            txt,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length = self.text_encoder_tokenizer.model_max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device) # Tokenize text\n",
    "\n",
    "        fBERT = self.text_encoder(**text_encoding).last_hidden_state[:, 0, :] # Use [CLS] token as text feature\n",
    "        # Shape: (B, 768)\n",
    "\n",
    "        # Compute ResNet Image Features\n",
    "        fResNet = self.image_encoder(img) # Output Shape: (B, 2048)\n",
    "\n",
    "        # Compute CLIP Text and Image Features\n",
    "        text_encoding = self.multimodal_encoder_tokenizer(\n",
    "            txt,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length = self.multimodal_encoder_tokenizer.model_max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device) # Tokenize text\n",
    "\n",
    "        fCLIP_T = self.multimodal_encoder.get_text_features(**text_encoding) # Compute CLIP Text Features\n",
    "        fCLIP_I = self.multimodal_encoder.get_image_features(img) # Compute CLIP Image Features\n",
    "\n",
    "        # Concatenate \n",
    "        fTxt = torch.cat((fBERT, fCLIP_T), dim = 1) # Shape: (B, 768 + 512 = 1280)\n",
    "        fImg = torch.cat((fResNet, fCLIP_I), dim = 1) # Shape: (B, 2048 + 512 = 2560)\n",
    "\n",
    "        # Compute mTxt and mImg\n",
    "        mTxt = self.pTxt(fTxt) # Shape: (B, 64)\n",
    "        mImg = self.pImg(fImg) # Shape: (B, 64)\n",
    "\n",
    "        fMix, mMix = self.compute_multimodal_features(fCLIP_T, fCLIP_I) \n",
    "        # fMix Shape: (B, 512 + 512 = 1024)\n",
    "        # mMix Shape: (B, 64)\n",
    "\n",
    "        # Perform Modality-Wise Attention\n",
    "        mAgg = self.attention(mTxt, mImg, mMix) # Shape: (B, 64)\n",
    "\n",
    "        # Compute Final Logits\n",
    "        logits = self.classification_head(mAgg) # Shape: (B, 2)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a779017d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FND_CLIP().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c899f00",
   "metadata": {},
   "source": [
    "## Set up Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69b3ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Class Proportions\n",
    "p0 = (TRAIN_DATA['2_way_label'] == 0).mean() # Computes the percentage of our training dataset that has label = 0 [Fake News]\n",
    "p1 = (TRAIN_DATA['2_way_label'] == 1).mean() # Computes the percentage of our training dataset that has label = 1 [Non-Fake News]\n",
    "print(f\"{p0  * 100}% of our dataset has label = 0 and {p1  * 100}% of our dataset has label = 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb964ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Weighted Loss Criterion\n",
    "class_weights = torch.tensor([p1, p0]).float().to(device)\n",
    "custom_criterion = nn.CrossEntropyLoss(weight = class_weights)\n",
    "print(f\"Class Weights: {class_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af525ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Adam Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd12959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=3):\n",
    "    model = model.to(device)\n",
    "\n",
    "    # --- simple bookkeeping lists ---\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    update = 0  # global update counter\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(\"-\" * 10)\n",
    "        model.train()\n",
    "        for batch_idx, (txts, imgs, labels) in enumerate(train_loader):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "            # ----- forward + backward -----\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(txts, imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # ----- training metrics -----\n",
    "            _, preds = outputs.max(1)\n",
    "            correct = (preds == labels).sum().item()\n",
    "            total = labels.size(0)\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            train_accuracies.append(correct / total)\n",
    "\n",
    "            print(f\"Batch {batch_idx + 1}/{len(train_loader)}, Loss: {train_losses[-1]}, Acc: {train_accuracies[-1]}\")\n",
    "\n",
    "            update += 1\n",
    "\n",
    "            # ----- every 100 updates: run validation + save -----\n",
    "            if update % 100 == 0 or update == 1:\n",
    "                model.eval()\n",
    "                v_correct = 0\n",
    "                v_total = 0\n",
    "                v_loss_total = 0.0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for v_txts, v_imgs, v_labels in val_loader:\n",
    "                        v_imgs, v_labels = v_imgs.to(device), v_labels.to(device)\n",
    "                        v_outputs = model(v_txts, v_imgs)\n",
    "                        v_loss = criterion(v_outputs, v_labels)\n",
    "\n",
    "                        _, v_preds = v_outputs.max(1)\n",
    "                        v_correct += (v_preds == v_labels).sum().item()\n",
    "                        v_total += v_labels.size(0)\n",
    "                        v_loss_total += v_loss.item() * v_labels.size(0)\n",
    "\n",
    "                val_accuracy = v_correct / v_total\n",
    "                val_loss = v_loss_total / v_total\n",
    "\n",
    "                # save model in Milestone 4 directory\n",
    "                save_dir = \"Milestone #4\"\n",
    "                if not os.path.exists(save_dir):\n",
    "                    os.makedirs(save_dir)\n",
    "\n",
    "                torch.save(model.state_dict(), f\"{save_dir}/latest_model.pt\")\n",
    "\n",
    "                # bookkeeping\n",
    "                val_losses.append(val_loss)\n",
    "                val_accuracies.append(val_accuracy)\n",
    "\n",
    "                # print everything\n",
    "                print(\n",
    "                    f\"[Update {update}] \"\n",
    "                    f\"Train Loss: {loss.item():.4f}, Train Acc: {correct/total:.4f} | \"\n",
    "                    f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\"\n",
    "                )\n",
    "\n",
    "                # Put model back in training mode\n",
    "                model.train()\n",
    "\n",
    "    save_dir = \"Milestone #4\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    torch.save(model.state_dict(), f\"{save_dir}/latest_model.pt\")\n",
    "    return train_losses, train_accuracies, val_losses, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e69594",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, train_accs, val_losses, val_accs = train_model(\n",
    "    model, train_loader, validation_loader, custom_criterion, optimizer, epochs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2141bcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
    "\n",
    "axs[0, 0].plot(train_losses)\n",
    "axs[0, 0].set_title(\"Train Losses\")\n",
    "\n",
    "axs[0, 1].plot(train_accs)\n",
    "axs[0, 1].set_title(\"Train Accuracies\")\n",
    "\n",
    "axs[1, 0].plot(val_losses)\n",
    "axs[1, 0].set_title(\"Validation Losses\")\n",
    "\n",
    "axs[1, 1].plot(val_accs)\n",
    "axs[1, 1].set_title(\"Validation Accuracies\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52e4c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Final Val Accuracy: {val_accs[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbf132e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smooth the train losses\n",
    "window = 10\n",
    "smooth_train_losses = pd.Series(train_losses).rolling(window=window, min_periods=1).mean()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_losses, alpha=0.3, label=\"Original Train Loss\")\n",
    "plt.plot(smooth_train_losses, color='red', label=\"Smoothed Train Loss\")\n",
    "plt.title(\"Train Losses (Original vs Smoothed)\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c923bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def compute_metrics(logits, labels):\n",
    "    logits = np.array(logits)\n",
    "    labels = np.array(labels)\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "\n",
    "        \"pos_precision\": precision_score(labels, predictions, pos_label=1, average=\"binary\", zero_division=0),\n",
    "        \"pos_recall\": recall_score(labels, predictions, pos_label=1, average=\"binary\", zero_division=0),\n",
    "        \"pos_f1\": f1_score(labels, predictions, pos_label=1, average=\"binary\", zero_division=0),\n",
    "\n",
    "        \"neg_precision\": precision_score(labels, predictions, pos_label=0, average=\"binary\", zero_division=0),\n",
    "        \"neg_recall\": recall_score(labels, predictions, pos_label=0, average=\"binary\", zero_division=0),\n",
    "        \"neg_f1\": f1_score(labels, predictions, pos_label=0, average=\"binary\", zero_division=0),\n",
    "\n",
    "        \"f1_macro\": f1_score(labels, predictions, average=\"macro\"),\n",
    "        \"f1_micro\": f1_score(labels, predictions, average=\"micro\"),\n",
    "        \"f1_weighted\": f1_score(labels, predictions, average=\"weighted\"),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b647528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for txts, imgs, labels in dataloader:\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(txts, imgs)      # logits\n",
    "            all_logits.append(outputs.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    all_logits = np.concatenate(all_logits, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    return compute_metrics(all_logits, all_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383d5992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- evaluate ----\n",
    "train_metrics = evaluate_model(model, train_loader)\n",
    "val_metrics   = evaluate_model(model, validation_loader)\n",
    "test_metrics  = evaluate_model(model, test_loader)\n",
    "\n",
    "# ---- save to CSV ----\n",
    "pd.DataFrame([train_metrics]).to_csv(\"FND_CLIP_train_metrics.csv\", index=False)\n",
    "pd.DataFrame([val_metrics]).to_csv(\"FND_CLIP_val_metrics.csv\", index=False)\n",
    "pd.DataFrame([test_metrics]).to_csv(\"FND_CLIP_test_metrics.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
