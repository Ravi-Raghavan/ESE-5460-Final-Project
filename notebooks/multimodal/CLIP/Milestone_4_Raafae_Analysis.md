# Milestone 4: FND CLIP Model with QKV Attention – Analysis

## Load Data & Setup Dataloaders
This section prepares the dataset and input pipeline used for evaluating the trained multimodal model. The training, validation, and test CSV files are loaded from disk, and rows corresponding to corrupted or missing images are removed using predefined index lists. This filtering step ensures that all remaining samples have valid image–text pairs and prevents runtime errors during image loading. Each sample is then assigned a zero-padded `image_num` identifier derived from its row index, which provides a consistent mapping between tabular data and image filenames stored in the image directories.

A custom `RedditDataset` class is defined to return triplets of raw text, preprocessed image tensors, and ground-truth labels. Image transformations are applied differently for training versus validation and testing, with augmentation used only for training data. A custom `collate_fn` is used in the DataLoader to batch variable-length text inputs together with fixed-size image tensors. The resulting DataLoaders provide batched multimodal inputs suitable for direct ingestion by the model during inference and attention analysis.

## QKV Attention Model Setup
This section defines the full Fake News Detection model built around modality-specific encoders and a learnable QKV-based attention mechanism. The architecture integrates three information streams: text features from a frozen BERT encoder, image features from a fine-tuned ResNet-101 backbone, and joint text–image features from a frozen CLIP model. Each modality’s raw features are passed through dedicated projection heads that map high-dimensional encoder outputs into a shared, low-dimensional embedding space suitable for attention-based fusion.

A custom QKV attention module is then applied across the three modality embeddings (text, image, and multimodal). Query, key, and value projections are learned, and multi-head attention computes how much each modality contributes relative to the others. The attention outputs are aggregated and passed through a final classification head to produce binary predictions. The model also includes a similarity-based weighting mechanism for multimodal features, using cosine similarity between CLIP text and image embeddings with running statistics to stabilize scaling. A pretrained checkpoint is loaded, and the model is placed in evaluation mode to ensure deterministic behavior during analysis.

## Sampled Attention Heatmaps
This section visualizes how the trained QKV attention mechanism distributes focus across text, image, and multimodal inputs for selected test samples. Precomputed attention scores, predicted labels, and ground-truth labels are loaded from disk. Samples are filtered to include only correctly classified examples where one modality’s attention score dominates the others by a specified margin, enabling targeted inspection of modality reliance.

For each selected sample, the corresponding image and text caption are displayed alongside a heatmap representing averaged attention weights over the three modalities. The attention weights are extracted directly from the model by performing a forward pass with `return_attention=True`, then averaging across heads and query positions. Three separate visualization sets are generated to highlight text-dominant, image-dominant, and multimodal-dominant decisions. Together, these plots provide qualitative insight into how the model adaptively prioritizes different information sources when making predictions.

