{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5aa3e2d6",
   "metadata": {},
   "source": [
    "# Milestone #2: Ravi Raghavan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee10cb88",
   "metadata": {},
   "source": [
    "## Milestone Aim\n",
    "\n",
    "The goal is to use a **pretrained BERT-Large, Uncased** model and **fine-tune it on the r/Fakeeddit dataset**.\n",
    "\n",
    "This work presents evaluation results on: \n",
    "- Pretrained BERT\n",
    "- Fine-Tuned BERT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010603eb",
   "metadata": {},
   "source": [
    "## Script Sanity Check\n",
    "\n",
    "Please ensure your directory is structured as follows\n",
    "\n",
    "```text\n",
    "cleaned_data/\n",
    "├── test.tsv\n",
    "├── train.tsv\n",
    "└── validation.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439c021f",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a2acde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "\n",
    "## Ensure TensorFlow is not used\n",
    "import os\n",
    "os.environ[\"USE_TF\"] = \"0\"\n",
    "\n",
    "# Import Hugging Face Tooling\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import evaluate\n",
    "from datasets import Dataset\n",
    "\n",
    "# Define data directory\n",
    "DATA_DIR = \"cleaned_data\"\n",
    "\n",
    "# Define file paths\n",
    "TRAIN_DATA_FILE = os.path.join(DATA_DIR, \"train.csv\")\n",
    "VALIDATION_DATA_FILE = os.path.join(DATA_DIR, \"validation.csv\")\n",
    "TEST_DATA_FILE = os.path.join(DATA_DIR, \"test.csv\")\n",
    "\n",
    "# For reproducability\n",
    "random_state = 42\n",
    "\n",
    "# Use CPU/MPS if possible\n",
    "device = None\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # Running in Colab\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "else:\n",
    "    # Not in Colab (e.g., Mac)\n",
    "    device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5de9611",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c462c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = pd.read_csv(TRAIN_DATA_FILE)\n",
    "VALIDATION_DATA = pd.read_csv(VALIDATION_DATA_FILE)\n",
    "TEST_DATA = pd.read_csv(TEST_DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eaef3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249d8404",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_DATA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b40b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9320f8a0",
   "metadata": {},
   "source": [
    "## Compute Class Proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffab1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Class Proportions\n",
    "p0 = (TRAIN_DATA['2_way_label'] == 0).mean() # Computes the percentage of our training dataset that has label = 0 [Fake News]\n",
    "p1 = (TRAIN_DATA['2_way_label'] == 1).mean() # Computes the percentage of our training dataset that has label = 1 [Non-Fake News]\n",
    "print(f\"{p0  * 100}% of our dataset has label = 0 and {p1  * 100}% of our dataset has label = 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646829c0",
   "metadata": {},
   "source": [
    "## Define Prior Adjusted Loss Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cd843b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Weighted Loss Criterion\n",
    "class_weights = torch.tensor([p1, p0]).float().to(device)\n",
    "custom_criterion = nn.CrossEntropyLoss(weight = class_weights)\n",
    "print(f\"Class Weights: {class_weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41c5719",
   "metadata": {},
   "source": [
    "## Fetch BERT From HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fca89b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch BERT Model from HuggingFace\n",
    "bert_model_name = 'bert-large-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(bert_model_name, num_labels = 2) # num_labels = 2 since we have 2 classes!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb60e77f",
   "metadata": {},
   "source": [
    "## Create `Hugging Face` Datasets [Train + Dev + Test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7105f299",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hf_dataset = Dataset.from_pandas(TRAIN_DATA)\n",
    "dev_hf_dataset = Dataset.from_pandas(VALIDATION_DATA)\n",
    "test_hf_dataset = Dataset.from_pandas(TEST_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454a2188",
   "metadata": {},
   "source": [
    "## Tokenize Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637e204c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(row):\n",
    "  tokens = tokenizer(row['clean_title'], truncation = True, padding = 'max_length', max_length = tokenizer.model_max_length)\n",
    "  row['input_ids'] = tokens['input_ids']\n",
    "  row['attention_mask'] = tokens['attention_mask']\n",
    "  row['token_type_ids'] = tokens['token_type_ids']\n",
    "  row['label'] = int(row['2_way_label'])\n",
    "  return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f0bd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hf_dataset = train_hf_dataset.map(tokenize_function)\n",
    "dev_hf_dataset = dev_hf_dataset.map(tokenize_function)\n",
    "test_hf_dataset = test_hf_dataset.map(tokenize_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bc838c",
   "metadata": {},
   "source": [
    "## Define Accuracy, Precision, Recall, and F1 Metrics from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bec9dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load('recall')\n",
    "f1_metric = evaluate.load(\"f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3c9b1c",
   "metadata": {},
   "source": [
    "## Define a compute_metrics function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495ce2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    # Get the model predictions\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Return Metrics\n",
    "    return {\n",
    "        \"accuracy\": accuracy_metric.compute(predictions=predictions, references=labels)['accuracy'], # Accuracy\n",
    "        \"pos_precision\": precision_metric.compute(predictions=predictions, references=labels, pos_label = 1, average = 'binary', zero_division = 0)[\"precision\"], # Precision on the Class w/ Label = 1 [Hate Samples]\n",
    "        \"pos_recall\": recall_metric.compute(predictions=predictions, references=labels, pos_label = 1, average = 'binary', zero_division = 0)['recall'], # Recall on the Class w/ Label = 1 [Hate Samples]\n",
    "        \"pos_f1\": f1_metric.compute(predictions=predictions, references=labels, pos_label = 1, average = 'binary')[\"f1\"], # F1 Score on the Class w/ Label = 1 [Hate Samples]\n",
    "        \"neg_precision\": precision_metric.compute(predictions=predictions, references=labels, pos_label = 0, average = 'binary', zero_division = 0)['precision'], # Precision on the Class w/ Label = 0 [Non-Hate Samples]\n",
    "        \"neg_recall\": recall_metric.compute(predictions=predictions, references=labels, pos_label = 0, average = 'binary', zero_division = 0)['recall'], # Recall on the Class w/ Label = 0 [Non-Hate Samples]\n",
    "        \"neg_f1\": f1_metric.compute(predictions=predictions, references=labels, pos_label = 0, average = 'binary')['f1'], # F1 Score on the Class w/ Label = 0 [Non-Hate Samples]\n",
    "        \"f1_macro\": f1_metric.compute(predictions=predictions, references=labels, average='macro')['f1'], # Macro F1 Score\n",
    "        \"f1_micro\": f1_metric.compute(predictions=predictions, references=labels, average='micro')['f1'], # Micro F1 Score\n",
    "        \"f1_weighted\": f1_metric.compute(predictions=predictions, references=labels, average='weighted')['f1'], # Weighted F1 Score\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26113c3",
   "metadata": {},
   "source": [
    "## Subclass the `Trainer` Class from HuggingFace to use Custom Loss Criterion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38626840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subclassed Trainer that enables us to use the custom loss function defined earlier\n",
    "class SubTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs = False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss = custom_criterion(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d630f6c",
   "metadata": {},
   "source": [
    "## **Initialize the `TrainingArguments` and `Trainer`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4785702d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"Milestone2-Baseline-BERT-FineTuning\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=3,\n",
    "    save_strategy=\"steps\",      # save checkpoints every N steps\n",
    "    save_steps=50,             # save every 50 steps\n",
    "    eval_strategy=\"steps\",      # evaluate every N steps\n",
    "    eval_steps=50,             # evaluate every 50 steps\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,          # log every 50 steps\n",
    "    report_to=\"none\",\n",
    "    full_determinism=True\n",
    ")\n",
    "\n",
    "trainer = SubTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_hf_dataset,\n",
    "    eval_dataset=dev_hf_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c3e91e",
   "metadata": {},
   "source": [
    "# **Evaluate Pre-Trained Model on Train, Dev, and Test Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b9add2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split: Train, Dev, or Test\n",
    "def generate_evaluation_results(split):\n",
    "    dataset = None\n",
    "    if split == \"train\":\n",
    "        dataset = train_hf_dataset\n",
    "    elif split == \"dev\" or split == \"validation\" or split == \"val\":\n",
    "        dataset = dev_hf_dataset\n",
    "    elif split == \"test\":\n",
    "        dataset = test_hf_dataset\n",
    "    \n",
    "    results = trainer.evaluate(eval_dataset=dataset, metric_key_prefix=split)\n",
    "    df_results = pd.DataFrame([results])\n",
    "    df_results.to_csv(f\"Milestone #2 Pre-Trained BERT Baseline {split} Results.csv\", index=False)\n",
    "    print(f\"Saved {split} evaluation metrics to Milestone #2 Pre-Trained BERT Baseline {split} Results.csv\")\n",
    "\n",
    "# Generate Evaluation Results on Train, Dev, and Test Splits\n",
    "generate_evaluation_results(\"train\")\n",
    "generate_evaluation_results(\"dev\")\n",
    "generate_evaluation_results(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba2d952",
   "metadata": {},
   "source": [
    "# **Train the Model: `Fine-Tuning`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee435fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train() # Always Resume from Last Checkpoint to Save Time\n",
    "trainer.save_model('Milestone2-Baseline-BERT-FinalModel') # Save the Final Model\n",
    "trainer.save_state() # Save the State of the Trainer (e.g. Losses, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71f7e60",
   "metadata": {},
   "source": [
    "# **Evaluate Fine-Tuned Model on Train, Dev, and Test Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aab0758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split: Train, Dev, or Test\n",
    "def generate_evaluation_results(split):\n",
    "    dataset = None\n",
    "    if split == \"train\":\n",
    "        dataset = train_hf_dataset\n",
    "    elif split == \"dev\" or split == \"validation\" or split == \"val\":\n",
    "        dataset = dev_hf_dataset\n",
    "    elif split == \"test\":\n",
    "        dataset = test_hf_dataset\n",
    "    \n",
    "    results = trainer.evaluate(eval_dataset=dataset, metric_key_prefix=split)\n",
    "    df_results = pd.DataFrame([results])\n",
    "    df_results.to_csv(f\"Milestone #2 Fine-Tuned BERT Baseline {split} Results.csv\", index=False)\n",
    "    print(f\"Saved {split} evaluation metrics to Milestone #2 Fine-Tuned BERT Baseline {split} Results.csv\")\n",
    "\n",
    "# Generate Evaluation Results on Train, Dev, and Test Splits\n",
    "generate_evaluation_results(\"train\")\n",
    "generate_evaluation_results(\"dev\")\n",
    "generate_evaluation_results(\"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
