{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5aa3e2d6",
   "metadata": {},
   "source": [
    "# Milestone #2: Ravi Raghavan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee10cb88",
   "metadata": {},
   "source": [
    "## Milestone Aim\n",
    "\n",
    "The goal is to use a **pretrained BERT-Base, Uncased** model and **fine-tune it on the r/Fakeeddit dataset**.\n",
    "\n",
    "This work presents evaluation results on: \n",
    "- Pretrained BERT\n",
    "- Fine-Tuned BERT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010603eb",
   "metadata": {},
   "source": [
    "## Script Sanity Check\n",
    "\n",
    "Please ensure your directory is structured as follows\n",
    "\n",
    "```text\n",
    "cleaned_data/\n",
    "├── test.csv\n",
    "├── train.csv\n",
    "└── validation.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439c021f",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19a2acde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "\n",
    "## Ensure TensorFlow is not used\n",
    "import os\n",
    "os.environ[\"USE_TF\"] = \"0\"\n",
    "\n",
    "# Import Hugging Face Tooling\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import evaluate\n",
    "from datasets import Dataset\n",
    "\n",
    "# Define data directory\n",
    "DATA_DIR = \"cleaned_data\"\n",
    "\n",
    "# Define file paths\n",
    "TRAIN_DATA_FILE = os.path.join(DATA_DIR, \"train.csv\")\n",
    "VALIDATION_DATA_FILE = os.path.join(DATA_DIR, \"validation.csv\")\n",
    "TEST_DATA_FILE = os.path.join(DATA_DIR, \"test.csv\")\n",
    "\n",
    "# For reproducability\n",
    "random_state = 42\n",
    "\n",
    "# Use CPU/MPS if possible\n",
    "device = None\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # Running in Colab\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "else:\n",
    "    # Not in Colab (e.g., Mac)\n",
    "    device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5de9611",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c462c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = pd.read_csv(TRAIN_DATA_FILE)\n",
    "VALIDATION_DATA = pd.read_csv(VALIDATION_DATA_FILE)\n",
    "TEST_DATA = pd.read_csv(TEST_DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1809b28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore rows in corrupted_indices.txt files\n",
    "def filter_out_corrupted_rows(split, DF):\n",
    "    # File with corrupted indices\n",
    "    corrupted_indices_file = f\"{split}_corrupted_indices.txt\"\n",
    "\n",
    "    # Store list of corrupted indices\n",
    "    corrupted_indices = None\n",
    "\n",
    "    # Get list of corrupted indices\n",
    "    with open(corrupted_indices_file, \"r\") as f:\n",
    "        corrupted_indices = list(int(line.strip()) for line in f if line.strip())\n",
    "    \n",
    "    print(f\"Split: {split}, Corrupted Indices: {corrupted_indices}, Length: {len(corrupted_indices)}\")\n",
    "    \n",
    "    # Filter out corrupted rows\n",
    "    DF = DF.drop(index = corrupted_indices)\n",
    "\n",
    "    return DF    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a89f79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split: train, Corrupted Indices: [2862, 26040, 28337, 18547, 13374, 11288, 31984, 18451, 19000, 22479, 8048, 32075, 22918, 5586, 19345, 12770, 32189, 14628, 9081, 6611, 2927], Length: 21\n",
      "Split: validation, Corrupted Indices: [14636, 6568, 30742, 19712, 6168, 5648, 23809, 24758, 6935, 21583, 20879, 32176, 10937, 25740, 16610, 15521, 673, 32574, 24115, 17734], Length: 20\n",
      "Split: test, Corrupted Indices: [4618, 33467, 17841, 15667, 12391, 19375, 18069, 7955, 10385, 29133, 6473, 9437, 14114, 17352, 26504, 11394, 17280, 32372, 9251, 23269], Length: 20\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA = filter_out_corrupted_rows(\"train\", TRAIN_DATA)\n",
    "VALIDATION_DATA = filter_out_corrupted_rows(\"validation\", VALIDATION_DATA)\n",
    "TEST_DATA = filter_out_corrupted_rows(\"test\", TEST_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30eaef3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_title</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>domain</th>\n",
       "      <th>image_url</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>2_way_label</th>\n",
       "      <th>3_way_label</th>\n",
       "      <th>6_way_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this spongebob squarepants branded battery</td>\n",
       "      <td>2019-07-30 20:00:50</td>\n",
       "      <td>i.redd.it</td>\n",
       "      <td>https://preview.redd.it/f39wxxk8yhd31.jpg?widt...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>33</td>\n",
       "      <td>mildlyinteresting</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>award for careless talk</td>\n",
       "      <td>2011-09-03 17:26:23</td>\n",
       "      <td>i.imgur.com</td>\n",
       "      <td>https://external-preview.redd.it/KgPHCi1u3fY5j...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14</td>\n",
       "      <td>propagandaposters</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>four aligned airplanes</td>\n",
       "      <td>2017-11-20 06:05:45</td>\n",
       "      <td>i.redd.it</td>\n",
       "      <td>https://preview.redd.it/88v9axk19phx.jpg?width...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>198</td>\n",
       "      <td>confusing_perspective</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>columbus discovers the new world</td>\n",
       "      <td>2019-08-28 15:40:17</td>\n",
       "      <td>i.redd.it</td>\n",
       "      <td>https://preview.redd.it/x4wzpd0am7j31.jpg?widt...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>318</td>\n",
       "      <td>fakehistoryporn</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>feed me drummmmssssssss</td>\n",
       "      <td>2014-05-09 13:23:59</td>\n",
       "      <td>i.imgur.com</td>\n",
       "      <td>https://external-preview.redd.it/yNN57loQnVhLk...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>pareidolia</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  clean_title          created_utc  \\\n",
       "0  this spongebob squarepants branded battery  2019-07-30 20:00:50   \n",
       "1                     award for careless talk  2011-09-03 17:26:23   \n",
       "2                      four aligned airplanes  2017-11-20 06:05:45   \n",
       "3            columbus discovers the new world  2019-08-28 15:40:17   \n",
       "4                     feed me drummmmssssssss  2014-05-09 13:23:59   \n",
       "\n",
       "        domain                                          image_url  \\\n",
       "0    i.redd.it  https://preview.redd.it/f39wxxk8yhd31.jpg?widt...   \n",
       "1  i.imgur.com  https://external-preview.redd.it/KgPHCi1u3fY5j...   \n",
       "2    i.redd.it  https://preview.redd.it/88v9axk19phx.jpg?width...   \n",
       "3    i.redd.it  https://preview.redd.it/x4wzpd0am7j31.jpg?widt...   \n",
       "4  i.imgur.com  https://external-preview.redd.it/yNN57loQnVhLk...   \n",
       "\n",
       "   num_comments  score              subreddit  upvote_ratio  2_way_label  \\\n",
       "0           4.0     33      mildlyinteresting          0.95            1   \n",
       "1           1.0     14      propagandaposters          1.00            0   \n",
       "2          24.0    198  confusing_perspective          0.98            0   \n",
       "3           5.0    318        fakehistoryporn          0.98            0   \n",
       "4           0.0      3             pareidolia          0.62            0   \n",
       "\n",
       "   3_way_label  6_way_label  \n",
       "0            0            0  \n",
       "1            1            5  \n",
       "2            2            2  \n",
       "3            2            2  \n",
       "4            2            2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DATA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "249d8404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_title</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>domain</th>\n",
       "      <th>image_url</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>2_way_label</th>\n",
       "      <th>3_way_label</th>\n",
       "      <th>6_way_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>driving toward the great pyramid of giza</td>\n",
       "      <td>2015-08-20 00:44:04</td>\n",
       "      <td>i.imgur.com</td>\n",
       "      <td>https://external-preview.redd.it/qOoqG_Bzmxrxx...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>misleadingthumbnails</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how to get charizard and other starters in pok...</td>\n",
       "      <td>2016-09-04 05:55:37</td>\n",
       "      <td>inverse.com</td>\n",
       "      <td>https://external-preview.redd.it/2HRiWyzJP1Obr...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>105</td>\n",
       "      <td>savedyouaclick</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my friend gave me this euro with this guy on i...</td>\n",
       "      <td>2019-05-13 14:23:20</td>\n",
       "      <td>i.redd.it</td>\n",
       "      <td>https://preview.redd.it/hty90t90nzx21.jpg?widt...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>38</td>\n",
       "      <td>mildlyinteresting</td>\n",
       "      <td>0.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ho chi minh visits the united states</td>\n",
       "      <td>2018-06-16 05:21:17</td>\n",
       "      <td>i.redd.it</td>\n",
       "      <td>https://preview.redd.it/iepetf7ksa411.jpg?widt...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27</td>\n",
       "      <td>fakehistoryporn</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bipartisan bill seeking deeper russiaputin pro...</td>\n",
       "      <td>2019-03-14 04:36:29</td>\n",
       "      <td>northcountrypublicradio.org</td>\n",
       "      <td>https://external-preview.redd.it/YObKn9FcqRgNa...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18</td>\n",
       "      <td>usnews</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         clean_title          created_utc  \\\n",
       "0           driving toward the great pyramid of giza  2015-08-20 00:44:04   \n",
       "1  how to get charizard and other starters in pok...  2016-09-04 05:55:37   \n",
       "2  my friend gave me this euro with this guy on i...  2019-05-13 14:23:20   \n",
       "3               ho chi minh visits the united states  2018-06-16 05:21:17   \n",
       "4  bipartisan bill seeking deeper russiaputin pro...  2019-03-14 04:36:29   \n",
       "\n",
       "                        domain  \\\n",
       "0                  i.imgur.com   \n",
       "1                  inverse.com   \n",
       "2                    i.redd.it   \n",
       "3                    i.redd.it   \n",
       "4  northcountrypublicradio.org   \n",
       "\n",
       "                                           image_url  num_comments  score  \\\n",
       "0  https://external-preview.redd.it/qOoqG_Bzmxrxx...           0.0      5   \n",
       "1  https://external-preview.redd.it/2HRiWyzJP1Obr...           1.0    105   \n",
       "2  https://preview.redd.it/hty90t90nzx21.jpg?widt...           1.0     38   \n",
       "3  https://preview.redd.it/iepetf7ksa411.jpg?widt...           0.0     27   \n",
       "4  https://external-preview.redd.it/YObKn9FcqRgNa...           1.0     18   \n",
       "\n",
       "              subreddit  upvote_ratio  2_way_label  3_way_label  6_way_label  \n",
       "0  misleadingthumbnails          0.65            0            2            2  \n",
       "1        savedyouaclick          0.94            0            2            5  \n",
       "2     mildlyinteresting          0.87            1            0            0  \n",
       "3       fakehistoryporn          0.92            0            2            2  \n",
       "4                usnews          0.96            1            0            0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VALIDATION_DATA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34b40b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_title</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>domain</th>\n",
       "      <th>image_url</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>2_way_label</th>\n",
       "      <th>3_way_label</th>\n",
       "      <th>6_way_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>us antiterror center helped police track envir...</td>\n",
       "      <td>2019-10-02 10:48:11</td>\n",
       "      <td>theguardian.com</td>\n",
       "      <td>https://external-preview.redd.it/D1BJCo8ZObamg...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26</td>\n",
       "      <td>usnews</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comey what can i say im just a catty bitch fro...</td>\n",
       "      <td>2018-04-16 17:02:44</td>\n",
       "      <td>politics.theonion.com</td>\n",
       "      <td>https://external-preview.redd.it/JDKJYZJX-LDW9...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>45</td>\n",
       "      <td>theonion</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>watch mom attacked over shopping cart in wi wa...</td>\n",
       "      <td>2018-07-29 04:37:30</td>\n",
       "      <td>wtvm.com</td>\n",
       "      <td>https://external-preview.redd.it/wRxI1DzMEAg-8...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>usnews</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a different kind of uplifting news maryland ac...</td>\n",
       "      <td>2018-08-15 03:11:45</td>\n",
       "      <td>espn.com</td>\n",
       "      <td>https://external-preview.redd.it/VaZiSdjw-4kF_...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13</td>\n",
       "      <td>upliftingnews</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this weimaraner at doggy care</td>\n",
       "      <td>2018-09-14 20:20:20</td>\n",
       "      <td>i.redd.it</td>\n",
       "      <td>https://preview.redd.it/4iwrppt0j9m11.jpg?widt...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13</td>\n",
       "      <td>photoshopbattles</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         clean_title          created_utc  \\\n",
       "0  us antiterror center helped police track envir...  2019-10-02 10:48:11   \n",
       "1  comey what can i say im just a catty bitch fro...  2018-04-16 17:02:44   \n",
       "2  watch mom attacked over shopping cart in wi wa...  2018-07-29 04:37:30   \n",
       "3  a different kind of uplifting news maryland ac...  2018-08-15 03:11:45   \n",
       "4                      this weimaraner at doggy care  2018-09-14 20:20:20   \n",
       "\n",
       "                  domain                                          image_url  \\\n",
       "0        theguardian.com  https://external-preview.redd.it/D1BJCo8ZObamg...   \n",
       "1  politics.theonion.com  https://external-preview.redd.it/JDKJYZJX-LDW9...   \n",
       "2               wtvm.com  https://external-preview.redd.it/wRxI1DzMEAg-8...   \n",
       "3               espn.com  https://external-preview.redd.it/VaZiSdjw-4kF_...   \n",
       "4              i.redd.it  https://preview.redd.it/4iwrppt0j9m11.jpg?widt...   \n",
       "\n",
       "   num_comments  score         subreddit  upvote_ratio  2_way_label  \\\n",
       "0           1.0     26            usnews          1.00            1   \n",
       "1           1.0     45          theonion          0.94            0   \n",
       "2           1.0      3            usnews          1.00            1   \n",
       "3           2.0     13     upliftingnews          0.88            1   \n",
       "4           5.0     13  photoshopbattles          0.90            1   \n",
       "\n",
       "   3_way_label  6_way_label  \n",
       "0            0            0  \n",
       "1            2            1  \n",
       "2            0            0  \n",
       "3            0            0  \n",
       "4            0            0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_DATA.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9320f8a0",
   "metadata": {},
   "source": [
    "## Compute Class Proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffab1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Class Proportions\n",
    "p0 = (TRAIN_DATA['2_way_label'] == 0).mean() # Computes the percentage of our training dataset that has label = 0 [Fake News]\n",
    "p1 = (TRAIN_DATA['2_way_label'] == 1).mean() # Computes the percentage of our training dataset that has label = 1 [Non-Fake News]\n",
    "print(f\"{p0  * 100}% of our dataset has label = 0 and {p1  * 100}% of our dataset has label = 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646829c0",
   "metadata": {},
   "source": [
    "## Define Prior Adjusted Loss Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cd843b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Weighted Loss Criterion\n",
    "class_weights = torch.tensor([p1, p0]).float().to(device)\n",
    "custom_criterion = nn.CrossEntropyLoss(weight = class_weights)\n",
    "print(f\"Class Weights: {class_weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41c5719",
   "metadata": {},
   "source": [
    "## Fetch BERT From HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fca89b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch BERT Model from HuggingFace\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(bert_model_name, num_labels = 2) # num_labels = 2 since we have 2 classes!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb60e77f",
   "metadata": {},
   "source": [
    "## Create `Hugging Face` Datasets [Train + Dev + Test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7105f299",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hf_dataset = Dataset.from_pandas(TRAIN_DATA)\n",
    "dev_hf_dataset = Dataset.from_pandas(VALIDATION_DATA)\n",
    "test_hf_dataset = Dataset.from_pandas(TEST_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454a2188",
   "metadata": {},
   "source": [
    "## Tokenize Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637e204c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(row):\n",
    "  tokens = tokenizer(row['clean_title'], truncation = True, padding = 'max_length', max_length = tokenizer.model_max_length)\n",
    "  row['input_ids'] = tokens['input_ids']\n",
    "  row['attention_mask'] = tokens['attention_mask']\n",
    "  row['token_type_ids'] = tokens['token_type_ids']\n",
    "  row['label'] = int(row['2_way_label'])\n",
    "  return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f0bd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hf_dataset = train_hf_dataset.map(tokenize_function)\n",
    "dev_hf_dataset = dev_hf_dataset.map(tokenize_function)\n",
    "test_hf_dataset = test_hf_dataset.map(tokenize_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bc838c",
   "metadata": {},
   "source": [
    "## Define Accuracy, Precision, Recall, and F1 Metrics from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bec9dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load('recall')\n",
    "f1_metric = evaluate.load(\"f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3c9b1c",
   "metadata": {},
   "source": [
    "## Define a compute_metrics function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495ce2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    # Get the model predictions\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Return Metrics\n",
    "    return {\n",
    "        \"accuracy\": accuracy_metric.compute(predictions=predictions, references=labels)['accuracy'], # Accuracy\n",
    "        \"pos_precision\": precision_metric.compute(predictions=predictions, references=labels, pos_label = 1, average = 'binary', zero_division = 0)[\"precision\"], # Precision on the Class w/ Label = 1 [Hate Samples]\n",
    "        \"pos_recall\": recall_metric.compute(predictions=predictions, references=labels, pos_label = 1, average = 'binary', zero_division = 0)['recall'], # Recall on the Class w/ Label = 1 [Hate Samples]\n",
    "        \"pos_f1\": f1_metric.compute(predictions=predictions, references=labels, pos_label = 1, average = 'binary')[\"f1\"], # F1 Score on the Class w/ Label = 1 [Hate Samples]\n",
    "        \"neg_precision\": precision_metric.compute(predictions=predictions, references=labels, pos_label = 0, average = 'binary', zero_division = 0)['precision'], # Precision on the Class w/ Label = 0 [Non-Hate Samples]\n",
    "        \"neg_recall\": recall_metric.compute(predictions=predictions, references=labels, pos_label = 0, average = 'binary', zero_division = 0)['recall'], # Recall on the Class w/ Label = 0 [Non-Hate Samples]\n",
    "        \"neg_f1\": f1_metric.compute(predictions=predictions, references=labels, pos_label = 0, average = 'binary')['f1'], # F1 Score on the Class w/ Label = 0 [Non-Hate Samples]\n",
    "        \"f1_macro\": f1_metric.compute(predictions=predictions, references=labels, average='macro')['f1'], # Macro F1 Score\n",
    "        \"f1_micro\": f1_metric.compute(predictions=predictions, references=labels, average='micro')['f1'], # Micro F1 Score\n",
    "        \"f1_weighted\": f1_metric.compute(predictions=predictions, references=labels, average='weighted')['f1'], # Weighted F1 Score\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26113c3",
   "metadata": {},
   "source": [
    "## Subclass the `Trainer` Class from HuggingFace to use Custom Loss Criterion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38626840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subclassed Trainer that enables us to use the custom loss function defined earlier\n",
    "class SubTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs = False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss = custom_criterion(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d630f6c",
   "metadata": {},
   "source": [
    "## **Initialize the `TrainingArguments` and `Trainer`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4785702d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"Milestone2-Baseline-BERT-FineTuning\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=3,\n",
    "    save_strategy=\"steps\",      # save checkpoints every N steps\n",
    "    save_steps=100,             # save every 100 steps\n",
    "    eval_strategy=\"steps\",      # evaluate every N steps\n",
    "    eval_steps=100,             # evaluate every 100 steps\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,          # log every 100 steps\n",
    "    report_to=\"none\",\n",
    "    full_determinism=True\n",
    ")\n",
    "\n",
    "trainer = SubTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_hf_dataset,\n",
    "    eval_dataset=dev_hf_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c3e91e",
   "metadata": {},
   "source": [
    "# **Evaluate Pre-Trained Model on Train, Dev, and Test Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b9add2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split: Train, Dev, or Test\n",
    "def generate_evaluation_results(split):\n",
    "    dataset = None\n",
    "    if split == \"train\":\n",
    "        dataset = train_hf_dataset\n",
    "    elif split == \"dev\" or split == \"validation\" or split == \"val\":\n",
    "        dataset = dev_hf_dataset\n",
    "    elif split == \"test\":\n",
    "        dataset = test_hf_dataset\n",
    "    \n",
    "    results = trainer.evaluate(eval_dataset=dataset, metric_key_prefix=split)\n",
    "    df_results = pd.DataFrame([results])\n",
    "    df_results.to_csv(f\"Milestone #2 Pre-Trained BERT Baseline {split} Results.csv\", index=False)\n",
    "    print(f\"Saved {split} evaluation metrics to Milestone #2 Pre-Trained BERT Baseline {split} Results.csv\")\n",
    "\n",
    "# Generate Evaluation Results on Train, Dev, and Test Splits\n",
    "generate_evaluation_results(\"train\")\n",
    "generate_evaluation_results(\"dev\")\n",
    "generate_evaluation_results(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba2d952",
   "metadata": {},
   "source": [
    "# **Train the Model: `Fine-Tuning`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee435fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train() # Always Resume from Last Checkpoint to Save Time\n",
    "trainer.save_model('Milestone2-Baseline-BERT-FinalModel') # Save the Final Model\n",
    "trainer.save_state() # Save the State of the Trainer (e.g. Losses, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71f7e60",
   "metadata": {},
   "source": [
    "# **Evaluate Fine-Tuned Model on Train, Dev, and Test Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aab0758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split: Train, Dev, or Test\n",
    "def generate_evaluation_results(split):\n",
    "    dataset = None\n",
    "    if split == \"train\":\n",
    "        dataset = train_hf_dataset\n",
    "    elif split == \"dev\" or split == \"validation\" or split == \"val\":\n",
    "        dataset = dev_hf_dataset\n",
    "    elif split == \"test\":\n",
    "        dataset = test_hf_dataset\n",
    "    \n",
    "    results = trainer.evaluate(eval_dataset=dataset, metric_key_prefix=split)\n",
    "    df_results = pd.DataFrame([results])\n",
    "    df_results.to_csv(f\"Milestone #2 Fine-Tuned BERT Baseline {split} Results.csv\", index=False)\n",
    "    print(f\"Saved {split} evaluation metrics to Milestone #2 Fine-Tuned BERT Baseline {split} Results.csv\")\n",
    "\n",
    "# Generate Evaluation Results on Train, Dev, and Test Splits\n",
    "generate_evaluation_results(\"train\")\n",
    "generate_evaluation_results(\"dev\")\n",
    "generate_evaluation_results(\"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
