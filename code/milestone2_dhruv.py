# -*- coding: utf-8 -*-
"""Milestone2_Dhruv.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WTOMIUmgpfKtWBVs6Lw90dIJW6SYfXjY
"""

# from google.colab import drive
# drive.mount('/content/drive')

# !unzip "/content/drive/MyDrive/ESE 5460 Term Project/Cleaned Data/Images/test_images.zip"

# !unzip "/content/drive/MyDrive/ESE 5460 Term Project/Cleaned Data/Images/train_images.zip"

# !unzip "/content/drive/MyDrive/ESE 5460 Term Project/Cleaned Data/Images/validation_images.zip"

"""# Changed this"""

train_data = "/content/drive/MyDrive/ESE 5460 Term Project/Cleaned Data/Text/train.csv"
test_data = "/content/drive/MyDrive/ESE 5460 Term Project/Cleaned Data/Text/test_5k.csv"
validation_data = "/content/drive/MyDrive/ESE 5460 Term Project/Cleaned Data/Text/validation_5k.csv"

# Ravi's Laptop
train_data = "cleaned_data/train.csv"
test_data = "cleaned_data/test_5k.csv"
validation_data = "cleaned_data/validation_5k.csv"

import pandas as pd

TRAIN_DATA = pd.read_csv(train_data)
VALIDATION_DATA = pd.read_csv(validation_data, index_col=0)
TEST_DATA = pd.read_csv(test_data, index_col=0)

TRAIN_DATA

"""# Getting image path from indexes"""

TRAIN_DATA['image_num'] = TRAIN_DATA.index.astype(str).str.zfill(5)
VALIDATION_DATA['image_num'] = VALIDATION_DATA.index.astype(str).str.zfill(5)
TEST_DATA['image_num'] = TEST_DATA.index.astype(str).str.zfill(5)

TRAIN_DATA

VALIDATION_DATA

# Ignore rows in corrupted_indices.txt files
def filter_out_corrupted_rows(split, DF):
    # File with corrupted indices
    # THIS PATH IS FOR DHRUV LAPTOP. Changed so ravi can train it :)
    if split == "train":
        corrupted_indices_file = f"/content/drive/MyDrive/ESE 5460 Term Project/Cleaned Data/New Corrupted File Information/{split}_corrupted_indices.txt"
        corrupted_indices_file = f"{split}_corrupted_indices.txt"

    else:
        corrupted_indices_file = f"/content/drive/MyDrive/ESE 5460 Term Project/Cleaned Data/New Corrupted File Information/{split}_5k_corrupted_indices.txt"
        corrupted_indices_file = f"{split}_5k_corrupted_indices.txt"

    # Store list of corrupted indices
    corrupted_indices = None

    # Get list of corrupted indices
    with open(corrupted_indices_file, "r") as f:
        corrupted_indices = list(int(line.strip()) for line in f if line.strip())

    print(f"Split: {split}, Corrupted Indices: {corrupted_indices}, Length: {len(corrupted_indices)}")

    # Filter out corrupted rows
    DF = DF.drop(index = corrupted_indices)

    return DF

TRAIN_DATA = filter_out_corrupted_rows("train", TRAIN_DATA)
VALIDATION_DATA = filter_out_corrupted_rows("validation", VALIDATION_DATA)
TEST_DATA = filter_out_corrupted_rows("test", TEST_DATA)

TRAIN_DATA

VALIDATION_DATA

from torch.utils.data import Dataset
from PIL import Image
import pandas as pd
import os

class ImageCSVDataset(Dataset):
    def __init__(self, df, img_dir, transform=None):
        self.data = df
        self.img_dir = img_dir
        self.transform = transform

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        img_name = self.data.iloc[idx]["image_num"] + ".jpg"
        label = self.data.iloc[idx]["2_way_label"]

        img_path = os.path.join(self.img_dir, img_name)
        image = Image.open(img_path).convert("RGB")

        if self.transform:
            image = self.transform(image)

        return image, label

from torchvision import transforms

train_tfms = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],
                         [0.229, 0.224, 0.225])
])

val_test_tfms = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],
                         [0.229, 0.224, 0.225])
])

# Changed img_dir paths
train_dataset = ImageCSVDataset(
    df=TRAIN_DATA,
    img_dir="train_images",
    transform=train_tfms
)

val_dataset = ImageCSVDataset(
    df=VALIDATION_DATA,
    img_dir="validation_images",
    transform=val_test_tfms
)

test_dataset = ImageCSVDataset(
    df=TEST_DATA,
    img_dir="test_images",
    transform=val_test_tfms
)

import torch
from torch.utils.data import DataLoader
import sys

# Use CPU/MPS if possible
device = None
if "google.colab" in sys.modules:
    # Running in Colab
    device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
else:
    # Not in Colab (e.g., Mac)
    device = torch.device("mps") if torch.backends.mps.is_available() else torch.device("cpu")

print("Using device:", device)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False)
test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False)

num_classes = TRAIN_DATA['2_way_label'].nunique()
num_classes

"""# Pre-trained ResNet101"""

import torch
import torch.nn as nn
from torchvision import models

model = models.resnet101(weights='IMAGENET1K_V1')

in_features = model.fc.in_features
model.fc = nn.Linear(in_features, num_classes)
model = model.to(device)

for param in model.parameters():
    param.requires_grad = False

for param in model.fc.parameters():
    param.requires_grad = True

model = model.to(device)

# Compute Class Proportions
p0 = (TRAIN_DATA['2_way_label'] == 0).mean() # Computes the percentage of our training dataset that has label = 0 [Fake News]
p1 = (TRAIN_DATA['2_way_label'] == 1).mean() # Computes the percentage of our training dataset that has label = 1 [Non-Fake News]
print(f"{p0  * 100}% of our dataset has label = 0 and {p1  * 100}% of our dataset has label = 1")

# Define Weighted Loss Criterion
class_weights = torch.tensor([p1, p0]).float().to(device)
criterion = nn.CrossEntropyLoss(weight = class_weights)
print(f"Class Weights: {class_weights}")

optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)  # Should we use the same lr and optimizer as BERT?

def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=3):
    model = model.to(device)

    # --- simple bookkeeping lists ---
    train_losses = []
    train_accuracies = []
    val_losses = []
    val_accuracies = []

    update = 0  # global update counter

    for epoch in range(epochs):
        print(f"Epoch {epoch+1}/{epochs}")
        print("-" * 10)
        model.train()
        for imgs, labels in train_loader:
            imgs, labels = imgs.to(device), labels.to(device)

            # ----- forward + backward -----
            optimizer.zero_grad()
            outputs = model(imgs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            # ----- training metrics -----
            _, preds = outputs.max(1)
            correct = (preds == labels).sum().item()
            total = labels.size(0)

            train_losses.append(loss.item())
            train_accuracies.append(correct / total)

            update += 1

            # ----- every 100 updates: run validation + save -----
            if update % 100 == 0:
                model.eval()
                v_correct = 0
                v_total = 0
                v_loss_total = 0.0

                with torch.no_grad():
                    for v_imgs, v_labels in val_loader:
                        v_imgs, v_labels = v_imgs.to(device), v_labels.to(device)
                        v_outputs = model(v_imgs)
                        v_loss = criterion(v_outputs, v_labels)

                        _, v_preds = v_outputs.max(1)
                        v_correct += (v_preds == v_labels).sum().item()
                        v_total += v_labels.size(0)
                        v_loss_total += v_loss.item() * v_labels.size(0)  # sum up batch loss

                val_accuracy = v_correct / v_total
                val_loss = v_loss_total / v_total

                # save model in Milestone 2 directory
                save_dir = "Milestone #2"
                if not os.path.exists(save_dir):
                    os.makedirs(save_dir)

                torch.save(model.state_dict(), f"{save_dir}/pretrained_latest_model.pt")

                # bookkeeping
                val_losses.append(val_loss)
                val_accuracies.append(val_accuracy)

                # print everything
                print(
                    f"[Update {update}] "
                    f"Train Loss: {loss.item():.4f}, Train Acc: {correct/total:.4f} | "
                    f"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}"
                )

                # Put back in training mode
                model.train()

    # save model in Milestone 2 directory
    save_dir = "Milestone #2"
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)

    torch.save(model.state_dict(), f"{save_dir}/pretrained_latest_model.pt")

    return train_losses, train_accuracies, val_losses, val_accuracies

train_losses, train_accs, val_losses, val_accs = train_model(
    model, train_loader, val_loader, criterion, optimizer, epochs=3
)

import pickle

with open("pretrained_metrics.pkl", "wb") as f:
    pickle.dump({
        "train_losses": train_losses,
        "train_accs": train_accs,
        "val_losses": val_losses,
        "val_accs": val_accs
    }, f)

import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

def compute_metrics(logits, labels):
    logits = np.array(logits)
    labels = np.array(labels)
    predictions = np.argmax(logits, axis=-1)

    return {
        "accuracy": accuracy_score(labels, predictions),

        "pos_precision": precision_score(labels, predictions, pos_label=1, average="binary", zero_division=0),
        "pos_recall": recall_score(labels, predictions, pos_label=1, average="binary", zero_division=0),
        "pos_f1": f1_score(labels, predictions, pos_label=1, average="binary", zero_division=0),

        "neg_precision": precision_score(labels, predictions, pos_label=0, average="binary", zero_division=0),
        "neg_recall": recall_score(labels, predictions, pos_label=0, average="binary", zero_division=0),
        "neg_f1": f1_score(labels, predictions, pos_label=0, average="binary", zero_division=0),

        "f1_macro": f1_score(labels, predictions, average="macro"),
        "f1_micro": f1_score(labels, predictions, average="micro"),
        "f1_weighted": f1_score(labels, predictions, average="weighted"),
    }

def evaluate_model(model, dataloader):
    model.eval()
    all_logits = []
    all_labels = []

    with torch.no_grad():
        for imgs, labels in dataloader:
            imgs = imgs.to(device)
            labels = labels.to(device)

            outputs = model(imgs)      # logits
            all_logits.append(outputs.cpu().numpy())
            all_labels.append(labels.cpu().numpy())

    all_logits = np.concatenate(all_logits, axis=0)
    all_labels = np.concatenate(all_labels, axis=0)

    return compute_metrics(all_logits, all_labels)

# ---- evaluate ----
train_metrics = evaluate_model(model, train_loader)
val_metrics   = evaluate_model(model, val_loader)
test_metrics  = evaluate_model(model, test_loader)

# ---- save to CSV ----
pd.DataFrame([train_metrics]).to_csv("pretrained_resnet_train_metrics.csv", index=False)
pd.DataFrame([val_metrics]).to_csv("pretrained_resnet_val_metrics.csv", index=False)
pd.DataFrame([test_metrics]).to_csv("pretrained_resnet_test_metrics.csv", index=False)

"""# Finetuned ResNet101"""

import torch
import torch.nn as nn
from torchvision import models

model = models.resnet101(weights='IMAGENET1K_V1')

in_features = model.fc.in_features
model.fc = nn.Linear(in_features, num_classes)
model = model.to(device)

for param in model.parameters():
    param.requires_grad = True

for param in model.fc.parameters():
    param.requires_grad = True

model = model.to(device)

# Compute Class Proportions
p0 = (TRAIN_DATA['2_way_label'] == 0).mean() # Computes the percentage of our training dataset that has label = 0 [Fake News]
p1 = (TRAIN_DATA['2_way_label'] == 1).mean() # Computes the percentage of our training dataset that has label = 1 [Non-Fake News]
print(f"{p0  * 100}% of our dataset has label = 0 and {p1  * 100}% of our dataset has label = 1")

# Define Weighted Loss Criterion
class_weights = torch.tensor([p1, p0]).float().to(device)
criterion = nn.CrossEntropyLoss(weight = class_weights)
print(f"Class Weights: {class_weights}")

optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)  # Should we use the same lr and optimizer as BERT?

def train_finetuned_model(model, train_loader, val_loader, criterion, optimizer, epochs=3):
    model = model.to(device)

    # --- simple bookkeeping lists ---
    train_losses = []
    train_accuracies = []
    val_losses = []
    val_accuracies = []

    update = 0  # global update counter

    for epoch in range(epochs):
        print(f"Epoch {epoch+1}/{epochs}")
        print("-" * 10)
        model.train()
        for imgs, labels in train_loader:
            imgs, labels = imgs.to(device), labels.to(device)

            # ----- forward + backward -----
            optimizer.zero_grad()
            outputs = model(imgs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            # ----- training metrics -----
            _, preds = outputs.max(1)
            correct = (preds == labels).sum().item()
            total = labels.size(0)

            train_losses.append(loss.item())
            train_accuracies.append(correct / total)

            update += 1

            # ----- every 100 updates: run validation + save -----
            if update % 100 == 0:
                model.eval()
                v_correct = 0
                v_total = 0
                v_loss_total = 0.0

                with torch.no_grad():
                    for v_imgs, v_labels in val_loader:
                        v_imgs, v_labels = v_imgs.to(device), v_labels.to(device)
                        v_outputs = model(v_imgs)
                        v_loss = criterion(v_outputs, v_labels)

                        _, v_preds = v_outputs.max(1)
                        v_correct += (v_preds == v_labels).sum().item()
                        v_total += v_labels.size(0)
                        v_loss_total += v_loss.item() * v_labels.size(0)

                val_accuracy = v_correct / v_total
                val_loss = v_loss_total / v_total

                # save model in Milestone 2 directory
                save_dir = "Milestone #2"
                if not os.path.exists(save_dir):
                    os.makedirs(save_dir)

                torch.save(model.state_dict(), f"{save_dir}/finetuned_latest_model.pt")

                # bookkeeping
                val_losses.append(val_loss)
                val_accuracies.append(val_accuracy)

                # print everything
                print(
                    f"[Update {update}] "
                    f"Train Loss: {loss.item():.4f}, Train Acc: {correct/total:.4f} | "
                    f"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}"
                )

                # Put back in train mode
                model.train()

    # save model in Milestone 2 directory
    save_dir = "Milestone #2"
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)

    torch.save(model.state_dict(), f"{save_dir}/finetuned_latest_model.pt")
    return train_losses, train_accuracies, val_losses, val_accuracies

train_losses, train_accs, val_losses, val_accs = train_finetuned_model(
    model, train_loader, val_loader, criterion, optimizer, epochs=3
)

import pickle

with open("finetuned_metrics.pkl", "wb") as f:
    pickle.dump({
        "train_losses": train_losses,
        "train_accs": train_accs,
        "val_losses": val_losses,
        "val_accs": val_accs
    }, f)

# ---- evaluate ----
train_metrics = evaluate_model(model, train_loader)
val_metrics   = evaluate_model(model, val_loader)
test_metrics  = evaluate_model(model, test_loader)

# ---- save to CSV ----
pd.DataFrame([train_metrics]).to_csv("finetuned_resnet_train_metrics.csv", index=False)
pd.DataFrame([val_metrics]).to_csv("finetuned_resnet_val_metrics.csv", index=False)
pd.DataFrame([test_metrics]).to_csv("finetuned_resnet_test_metrics.csv", index=False)

