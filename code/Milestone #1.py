# -*- coding: utf-8 -*-
"""Milestone #1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1flNPu_I2Z7SwBpDrsfO6IPJ5J6YRC69I

# Milestone #1: Data Preparation + Exploratory Data Analysis

## Dataset Access

Following the instructions from the [r/Fakeddit paper](https://arxiv.org/pdf/1911.03854), we obtained the dataset from the official [Fakeddit GitHub repository](https://github.com/entitize/fakeddit).  
The repository provides a link to the dataset’s Google Drive folder:  
<https://drive.google.com/drive/folders/1jU7qgDqU1je9Y0PMKJ_f31yXRo5uWGFm?usp=sharing>

Since our project focuses on **multimodal analysis**, we use only the multimodal samples, which contain **both text and images**.

## Script Instructions

To run this script, please download the following data files from the Google Drive link provided above:

- `multimodal_test_public.tsv`  
- `multimodal_train.tsv`  
- `multimodal_validate.tsv`  

Then, organize your local directory as follows:

```text
data/
├── multimodal_test_public.tsv
├── multimodal_train.tsv
└── multimodal_validate.tsv

## Environment Setup
"""

import pandas as pd
import numpy as np
import torch
import requests
from PIL import Image
from io import BytesIO
import os
from sklearn.model_selection import train_test_split
from tqdm import tqdm

# Define data directory
DATA_DIR = "data"

# Define file paths
TRAIN_DATA_FILE = os.path.join(DATA_DIR, "multimodal_train.tsv")
VALIDATION_DATA_FILE = os.path.join(DATA_DIR, "multimodal_validate.tsv")
TEST_DATA_FILE = os.path.join(DATA_DIR, "multimodal_test_public.tsv")

# For reproducability
random_state = 42

"""## Load Data"""

TRAIN_DATA = pd.read_csv(TRAIN_DATA_FILE, sep="\t")
VALIDATION_DATA = pd.read_csv(VALIDATION_DATA_FILE, sep="\t")
TEST_DATA = pd.read_csv(TEST_DATA_FILE, sep="\t")

TRAIN_DATA.head()

TRAIN_DATA.info()

VALIDATION_DATA.head()

VALIDATION_DATA.info()

TEST_DATA.head()

TEST_DATA.info()

"""## Feature Selection and Rationale

Our dataset contains several columns from Reddit posts. Below is a summary of why we are keeping or dropping certain features for our analysis and modeling:

| Column Name             | Action      | Reason |
|-------------------------|------------|--------|
| `author`                | Drop       | Not relevant for fake news analysis. The author's identity does not provide information about the content or veracity of the post, and Reddit usernames can be arbitrary.|
| `clean_title`           | Keep       | Already cleaned for us in the r/Fakeddit paper. Represents the text content of the post, essential for NLP analysis. |
| `created_utc`           | Keep       | Useful for downstream temporal analysis, e.g., examining when fake news spikes over time. |
| `domain`                | Keep       | Can help explore if posts from certain domains are more or less likely to be fake news. |
| `hasImage`              | Keep [Drop Eventually]       | Indicates if the post contains an image; Will be dropped after Sanity Checks[Refer to Sanity Check Section] |
| `id`                    | Drop       | Unique identifier, not informative for modeling. |
| `image_url`             | Keep       | Necessary to access image data for multimodal modeling. |
| `linked_submission_id`  | Drop       | Mostly missing and not relevant for our analysis. |
| `num_comments`          | Keep       | Can provide insights into engagement and post virality. |
| `score`                 | Keep       | Represents post popularity; potentially correlates with the spread of fake news. |
| `subreddit`             | Keep       | Useful for understanding community context and post categorization. |
| `title`                 | Drop       | Original title is redundant with `clean_title`. |
| `upvote_ratio`          | Keep       | Indicates community approval; may provide signals for fake vs real news. |
| `2_way_label`, `3_way_label`, `6_way_label` | Keep | These are the target labels used for classification tasks. |

In summary, we drop columns that are either identifiers (`id`, `linked_submission_id`), redundant (`title`), or not informative for fake news detection (`author`). We retain features that provide textual, temporal, engagement, or community context, as well as the labels needed for our deep learning tasks.

"""

RELEVANT_COLUMNS = ['clean_title', 'created_utc', 'domain', 'hasImage', 'image_url', 'num_comments',
                    'score', 'subreddit', 'upvote_ratio', '2_way_label', '3_way_label', '6_way_label']

# Pick Appropriate Subset of Train Data, Validation Data, and Test Data
TRAIN_DATA = TRAIN_DATA[RELEVANT_COLUMNS]
VALIDATION_DATA = VALIDATION_DATA[RELEVANT_COLUMNS]
TEST_DATA = TEST_DATA[RELEVANT_COLUMNS]

"""## Sanity Checks

Before proceeding with our analysis, we perform the following checks to ensure data quality for our **multimodal dataset**:

1. **Remove incomplete samples**: Drop any rows where `clean_title` or `image_url` is null.  
2. **Verify image availability**: Confirm that `hasImage` is `True` for all remaining samples, then remove the `hasImage` column.  
3. **Confirm labeling**: Ensure every sample has a label for our supervised learning task.
"""

def sanity_checks(DATA: pd.DataFrame):
    # 1. Drop rows with null clean_title or image_url
    DATA = DATA.dropna(axis = 'index', subset=['clean_title', 'image_url'], how = 'any')

    # 2. Keep only rows where hasImage is True, then drop the column
    DATA = DATA[DATA['hasImage']].drop(columns=['hasImage'])

    # 3. Drop rows where any label is missing
    label_columns = ['2_way_label', '3_way_label', '6_way_label']
    DATA = DATA.dropna(axis = 'index', subset = label_columns, how = 'any')

    # 4. Reset Index
    DATA = DATA.reset_index(drop=True)

    return DATA

# Perform Sanity Checks on ALL splits of data
TRAIN_DATA = sanity_checks(TRAIN_DATA)
VALIDATION_DATA = sanity_checks(VALIDATION_DATA)
TEST_DATA = sanity_checks(TEST_DATA)

TRAIN_DATA.info()

VALIDATION_DATA.info()

TEST_DATA.info()

"""## Convert Data Types

We convert the columns in our dataset to appropriate data types to ensure consistency and facilitate analysis:

- `clean_title`: string  
- `created_utc`: convert from UTC timestamp to `datetime`  
- `domain`: string  
- `image_url`: string  
- `subreddit`: string

"""

def convert_data_types(DATA: pd.DataFrame):
    DATA['clean_title'] = DATA['clean_title'].astype('string')
    DATA['created_utc'] = pd.to_datetime(DATA['created_utc'], unit='s')
    DATA['domain'] = DATA['domain'].astype('string')
    DATA['image_url'] = DATA['image_url'].astype('string')
    DATA['subreddit'] = DATA['subreddit'].astype('string')
    return DATA

# Perform Data Conversions on ALL splits of data
TRAIN_DATA = convert_data_types(TRAIN_DATA)
VALIDATION_DATA = convert_data_types(VALIDATION_DATA)
TEST_DATA = convert_data_types(TEST_DATA)

TRAIN_DATA.info()

TRAIN_DATA.head()

VALIDATION_DATA.info()

VALIDATION_DATA.head()

TEST_DATA.info()

TEST_DATA.head()

"""## Sample Dataset

Due to compute limitations, we subsample our dataset to a smaller size for training and evaluation:

- **Training set:** 25,000 samples  
- **Validation set:** 10,000 samples  
- **Test set:** 10,000 samples  

We ensure that the subsampling is **stratified** based on the target label to preserve the class distribution.
"""

# Stratified split for Train
TRAIN_DATA, _ = train_test_split(
    TRAIN_DATA,
    train_size=50000,
    stratify=TRAIN_DATA['2_way_label'],
    random_state = random_state
)

# Reset Index
TRAIN_DATA = TRAIN_DATA.reset_index(drop = True)

# Stratified split for Validation
VALIDATION_DATA, _ = train_test_split(
    VALIDATION_DATA,
    train_size=50000,
    stratify=VALIDATION_DATA['2_way_label'],
    random_state = random_state
)

# Reset Index
VALIDATION_DATA = VALIDATION_DATA.reset_index(drop = True)

# Stratified split for train
TEST_DATA, _ = train_test_split(
    TEST_DATA,
    train_size=50000,
    stratify=TEST_DATA['2_way_label'],
    random_state = random_state
)

# Reset Index
TEST_DATA = TEST_DATA.reset_index(drop = True)

print(f"Train samples: {len(TRAIN_DATA)}, Validation samples: {len(VALIDATION_DATA)}, Test samples: {len(TEST_DATA)}")

"""## Scrape Images + Save"""

def download_image(url, save_path):
    """
    Downloads an image from a URL and saves it locally.
    Returns True if successful, False if failed.
    """
    try:
        response = requests.get(url, timeout=10, stream=True)
        if response.status_code == 200:
            with open(save_path, "wb") as f:
                for chunk in response.iter_content(1024):
                    f.write(chunk)
            return True
        else:
            return False
    except Exception:
        return False

def download_split_images(df, out_dir):
    """
    Goes through each row in the dataframe,
    downloads the image in `image_url`,
    and saves it into the directory.
    """

    if not os.path.exists(out_dir):
        os.makedirs(out_dir)

    total = len(df)
    success_count = 0
    log_every = 100
    count = 0
    successful_rows = []
    print(f"\nStarting download for: {out_dir} ({total} images)\n")

    for _, row in tqdm(df.iterrows(), total=total, desc=f"Downloading → {out_dir}"):
        url = row["image_url"]
        filename = f"{success_count:05d}.jpg"
        save_path = os.path.join(out_dir, filename)
        success = download_image(url, save_path)

        if success:
            success_count += 1
            successful_rows.append(row)

        count += 1

        # Print running success count
        if count % log_every == 0:
            success_pct = (success_count / count) * 100
            fail_pct = ((count - success_count) / count) * 100
            print(f"[{success_pct:.2f}%] Successes  |  [{fail_pct:.2f}%] Failures")

    print(f"\nFinished {out_dir}: {success_count}/{total} images downloaded successfully.\n")
    return pd.DataFrame(successful_rows)

# Run for all splits
TRAIN_DATA = download_split_images(TRAIN_DATA, "train_images")
VALIDATION_DATA = download_split_images(VALIDATION_DATA, "validation_images")
TEST_DATA = download_split_images(TEST_DATA, "test_images")

"""## Save Cleaned Data"""

# Create local folder to save CSVs
local_dir = "cleaned_data"
if not os.path.exists(local_dir):
    os.makedirs(local_dir)

# Save CSVs
TRAIN_DATA.to_csv(os.path.join(local_dir, "train.csv"), index=False)
VALIDATION_DATA.to_csv(os.path.join(local_dir, "validation.csv"), index=False)
TEST_DATA.to_csv(os.path.join(local_dir, "test.csv"), index=False)

print(f"Training Size: {len(TRAIN_DATA)}, Validation Size: {len(VALIDATION_DATA)}, Test Size: {len(TEST_DATA)}")